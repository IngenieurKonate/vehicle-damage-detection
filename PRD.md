# üöó PRD ‚Äî D√©tection Automatique de Dommages sur V√©hicules par CNN From Scratch

> **Document de R√©f√©rence pour l'Impl√©mentation**  

---

## üìã Table des Mati√®res

1. [Vue d'Ensemble du Projet](#1-vue-densemble-du-projet)
2. [Contexte Acad√©mique et Contraintes](#2-contexte-acad√©mique-et-contraintes)
3. [Probl√©matique et Hypoth√®ses](#3-probl√©matique-et-hypoth√®ses)
4. [Sp√©cifications des Donn√©es](#4-sp√©cifications-des-donn√©es)
5. [Architecture Model A ‚Äî Baseline VGG-like](#5-architecture-model-a--baseline-vgg-like)
6. [Architecture Model B ‚Äî Deep CNN avec Skip Connections](#6-architecture-model-b--deep-cnn-avec-skip-connections)
7. [Pipeline d'Entra√Ænement](#7-pipeline-dentra√Ænement)
8. [Protocole d'√âvaluation](#8-protocole-d√©valuation)
9. [Structure du Projet](#9-structure-du-projet)
10. [Sp√©cifications d'Impl√©mentation](#10-sp√©cifications-dimpl√©mentation)
11. [Checklist de Validation](#11-checklist-de-validation)
12. [Glossaire Technique](#12-glossaire-technique)

---

## 1. Vue d'Ensemble du Projet

### 1.1 R√©sum√© Ex√©cutif

Ce projet vise √† concevoir, impl√©menter et comparer **deux architectures CNN from scratch** pour la d√©tection automatique de dommages visuels sur v√©hicules (rayures, bosses, fissures). L'objectif est de d√©montrer une **compr√©hension profonde** des concepts de Deep Learning √† travers une d√©marche de conception originale.

### 1.2 Objectifs du Projet

| ID | Objectif | Priorit√© | Crit√®re de Succ√®s |
|----|----------|----------|-------------------|
| O1 | Concevoir une architecture baseline (VGG-like) from scratch | üî¥ Critique | Architecture fonctionnelle, F1 ‚â• 0.70 |
| O2 | Concevoir une architecture deep avec skip connections | üî¥ Critique | Architecture fonctionnelle, ŒîF1 ‚â• +0.05 vs baseline |
| O3 | Comparer scientifiquement les deux architectures | üî¥ Critique | Analyse comparative document√©e |
| O4 | Produire un rapport acad√©mique rigoureux | üü† Important | Justification de chaque choix architectural |
| O5 | Cr√©er une pr√©sentation PowerPoint professionnelle | üü† Important | Slides clairs, visuels, d√©fense des choix |
| O6 | D√©velopper une application Flask de d√©monstration | üü¢ Secondaire | Interface web fonctionnelle pour pr√©diction |
| O7 | Impl√©menter la g√©n√©ration automatique de rapports | üü¢ Secondaire | PDF de diagnostic g√©n√©r√© automatiquement |

### 1.3 Port√©e (Scope)

#### ‚úÖ In-Scope

- Classification binaire : `damaged` vs `undamaged`
- Classification multi-classes (optionnel) : `scratch`, `dent`, `crack`, `shatter`, `undamaged`
- Deux architectures CNN con√ßues from scratch
- Pipeline complet : pr√©traitement ‚Üí entra√Ænement ‚Üí √©valuation
- Notebooks reproductibles et document√©s
- Rapport acad√©mique et pr√©sentation PowerPoint
- **[Secondaire]** Application Flask de d√©monstration (upload image ‚Üí pr√©diction)
- **[Secondaire]** G√©n√©ration automatique de rapports PDF de diagnostic

#### ‚ùå Out-of-Scope

- D√©tection avec bounding boxes (YOLO-style) ‚Äî hors p√©rim√®tre initial
- Segmentation s√©mantique des dommages
- D√©ploiement en production cloud (API scalable, CI/CD)
- Comparaison avant/apr√®s location automatis√©e
- Transfer learning avec mod√®les pr√©-entra√Æn√©s (interdit acad√©miquement)

---

## 2. Contexte Acad√©mique et Contraintes

### 2.1 Exigences du Professeur

> **Citation cl√© du professeur :**  
> *"Minimum one model implemented by the members of the team from scratch. Take ideas like the VGG block or residual connection and build your own model."*

### 2.2 Ce qui est AUTORIS√â ‚úÖ

```python
# ‚úÖ Utilisation de PyTorch/TensorFlow
import torch
import torch.nn as nn

# ‚úÖ Couches de base
nn.Conv2d, nn.Linear, nn.BatchNorm2d, nn.Dropout

# ‚úÖ Fonctions d'activation
nn.ReLU, nn.LeakyReLU, nn.Sigmoid, nn.Softmax

# ‚úÖ Pooling
nn.MaxPool2d, nn.AvgPool2d, nn.AdaptiveAvgPool2d

# ‚úÖ Optimiseurs et Loss
torch.optim.Adam, torch.optim.SGD
nn.CrossEntropyLoss, nn.BCELoss

# ‚úÖ Autograd
# Le calcul automatique des gradients est autoris√©

# ‚úÖ Data augmentation
torchvision.transforms.*
```

### 2.3 Ce qui est INTERDIT ‚ùå

```python
# ‚ùå Import de mod√®les pr√©-d√©finis
from torchvision.models import resnet18, vgg16, efficientnet_b0

# ‚ùå Mod√®les pr√©-entra√Æn√©s
model = resnet18(pretrained=True)  # INTERDIT
model = resnet18(weights=None)     # INTERDIT aussi (architecture pas la n√¥tre)

# ‚ùå Hubs de mod√®les
torch.hub.load('pytorch/vision', 'resnet18')
timm.create_model('efficientnet_b0')
```

### 2.4 Crit√®res de Notation (Implicites)

| Crit√®re | Poids Estim√© | Comment l'Atteindre |
|---------|--------------|---------------------|
| Compr√©hension architecturale | 30% | Justifier CHAQUE choix de couche |
| Originalit√© de conception | 25% | Architecture propre, pas copier-coller |
| Rigueur exp√©rimentale | 20% | Protocole clair, r√©sultats reproductibles |
| Qualit√© du code | 15% | Clean code, modulaire, document√© |
| Rapport final et pr√©sentation | 10% | Clart√©, rigueur, qualit√© des visuels |

---

## 3. Probl√©matique et Hypoth√®ses

### 3.1 Probl√©matique de Recherche

> **Question principale :**  
> Dans quelle mesure une architecture CNN con√ßue from scratch, s'inspirant des principes de VGG et ResNet, peut-elle d√©tecter efficacement les dommages visuels sur v√©hicules ?

> **Questions secondaires :**
> 1. Quel est l'apport mesurable des connexions r√©siduelles sur cette t√¢che ?
> 2. Quelle profondeur de r√©seau est optimale pour ce probl√®me sp√©cifique ?
> 3. Comment la data augmentation influence-t-elle la g√©n√©ralisation ?

### 3.2 Hypoth√®ses Exp√©rimentales

| ID | Hypoth√®se | Variable Ind√©pendante | Variable D√©pendante | Validation |
|----|-----------|----------------------|---------------------|------------|
| H1 | Un CNN VGG-like de 6-8 couches convolutives peut atteindre F1 ‚â• 0.70 sur la classification de dommages | Architecture (baseline) | F1-Score | Entra√Ænement Model A |
| H2 | L'ajout de skip connections am√©liore le F1-Score d'au moins 5 points | Pr√©sence de skip connections | F1-Score | Comparaison A vs B |
| H3 | BatchNorm acc√©l√®re la convergence et am√©liore la stabilit√© | Pr√©sence de BatchNorm | Loss convergence, variance | Ablation study |
| H4 | L'augmentation de donn√©es r√©duit l'√©cart train/val loss d'au moins 20% | Data augmentation | G√©n√©ralisation gap | Comparaison avec/sans augmentation |

### 3.3 Contribution Scientifique Attendue

Ce projet ne vise pas √† battre l'√©tat de l'art mais √† **d√©montrer** :

1. **Ma√Ætrise conceptuelle** : comprendre pourquoi certaines architectures fonctionnent
2. **Capacit√© de conception** : cr√©er une architecture adapt√©e au probl√®me
3. **Rigueur exp√©rimentale** : comparer objectivement deux approches
4. **Communication scientifique** : expliquer clairement des choix complexes

---

## 4. Sp√©cifications des Donn√©es

### 4.1 Strat√©gie de Dataset : Le Duo Gagnant

Pour r√©aliser une **classification binaire** (damaged vs undamaged), nous combinons **deux datasets compl√©mentaires** de r√©f√©rence acad√©mique.

#### Pourquoi deux datasets ?

Le dataset CarDD contient uniquement des images de v√©hicules endommag√©s. Pour entra√Æner un classificateur binaire, le mod√®le doit apprendre √† distinguer les deux classes. Sans images de v√©hicules en bon √©tat, le mod√®le pr√©dirait syst√©matiquement "damaged" (biais total).

```
DATASET COMBIN√â = CarDD (damaged) + Stanford Cars (undamaged)
                      ‚Üì                      ‚Üì
              Classe "DAMAGED"      Classe "UNDAMAGED"
                (4,000 images)       (4,000 images)
```

---

### 4.2 Dataset 1 : CarDD (V√©hicules Endommag√©s)

#### Informations G√©n√©rales

| Attribut | Valeur |
|----------|--------|
| **Nom complet** | Car Damage Detection Dataset (CarDD) |
| **Source** | USTC (University of Science and Technology of China) |
| **Publication** | IEEE Transactions on Intelligent Transportation Systems, 2023 |
| **Auteurs** | Wang, Xinkuang; Li, Wenjing; Wu, Zhongcheng |
| **Images** | 4,000 images haute r√©solution |
| **Instances annot√©es** | ~9,000 (plusieurs dommages par image) |
| **R√©solution moyenne** | 684,231 pixels (~13.6√ó sup√©rieure aux autres datasets) |
| **Taille totale** | ~5 GB (images + annotations + SOD) |
| **Format** | JPEG/PNG, RGB |

#### Structure du Dataset CarDD (t√©l√©charg√©)

```
CarDD_release/
‚îÇ
‚îú‚îÄ‚îÄ üìÅ CarDD_COCO/                    # ‚úÖ FORMAT COCO - √Ä UTILISER
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ annotations/               # ‚ùå Ignorer (fichiers JSON pour d√©tection)
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ train2017/                 # ‚úÖ 2,816 images
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ val2017/                   # ‚úÖ 810 images
‚îÇ   ‚îî‚îÄ‚îÄ üìÅ test2017/                  # ‚úÖ ~374 images
‚îÇ
‚îî‚îÄ‚îÄ üìÅ CarDD_SOD/                     # ‚ùå IGNORER ENTI√àREMENT
    ‚îú‚îÄ‚îÄ üìÅ CarDD-TE/                  # (Salient Object Detection - autre t√¢che)
    ‚îú‚îÄ‚îÄ üìÅ CarDD-TR/
    ‚îî‚îÄ‚îÄ üìÅ CarDD-VAL/
```

#### Distribution des Images CarDD

| Split | Nombre d'images | Pourcentage |
|-------|-----------------|-------------|
| **train2017** | 2,816 | 70.4% |
| **val2017** | 810 | 20.25% |
| **test2017** | ~374 | 9.35% |
| **TOTAL** | **~4,000** | 100% |

#### Ce qu'on utilise vs ce qu'on ignore

| √âl√©ment | Taille estim√©e | Utilisation |
|---------|----------------|-------------|
| `CarDD_COCO/train2017/` | ~2 GB | ‚úÖ **UTILISER** |
| `CarDD_COCO/val2017/` | ~600 MB | ‚úÖ **UTILISER** |
| `CarDD_COCO/test2017/` | ~300 MB | ‚úÖ **UTILISER** |
| `CarDD_COCO/annotations/` | ~50 MB | ‚ùå Ignorer (JSON pour YOLO/Mask R-CNN) |
| `CarDD_SOD/` | ~2 GB | ‚ùå Ignorer (autre t√¢che) |

> **Note** : Pour notre classification binaire, seules les **images** sont n√©cessaires. Les annotations COCO (bounding boxes, masques) et le dossier SOD ne sont pas utilis√©s car nous ne faisons pas de d√©tection d'objets ni de segmentation.

#### Cat√©gories de Dommages (6 classes)

| Cat√©gorie | Traduction | Description |
|-----------|------------|-------------|
| `dent` | Bosse | D√©formation du m√©tal de carrosserie |
| `scratch` | Rayure | Dommage superficiel de la peinture |
| `crack` | Fissure | Fracture profonde du mat√©riau |
| `glass shatter` | Vitre bris√©e | Pare-brise ou vitres cass√©s |
| `lamp broken` | Phare cass√© | Optiques avant/arri√®re endommag√©es |
| `tire flat` | Pneu crev√© | Pneumatique √† plat |

#### Liens de R√©f√©rence

| Ressource | URL |
|-----------|-----|
| **Site officiel** | https://cardd-ustc.github.io/ |
| **Paper ArXiv** | https://arxiv.org/abs/2211.00945 |
| **Paper IEEE** | https://ieeexplore.ieee.org/document/10078726 |
| **GitHub** | https://github.com/CarDD-USTC/CarDD-USTC.github.io |
| **Hugging Face** | https://huggingface.co/datasets/harpreetsahota/CarDD |

#### Citation BibTeX

```bibtex
@article{CarDD,
    author={Wang, Xinkuang and Li, Wenjing and Wu, Zhongcheng},
    journal={IEEE Transactions on Intelligent Transportation Systems},
    title={CarDD: A New Dataset for Vision-Based Car Damage Detection},
    year={2023},
    volume={24},
    number={7},
    pages={7202-7214},
    doi={10.1109/TITS.2023.3258480}
}
```

#### Utilisation dans notre projet

- **R√¥le** : Fournir la classe `DAMAGED`
- **S√©lection** : Toutes les 4,000 images (ind√©pendamment du type de dommage)
- **Label assign√©** : `1` (damaged)

---

### 4.3 Dataset 2 : Stanford Cars (V√©hicules Non Endommag√©s)

#### Informations G√©n√©rales

| Attribut | Valeur |
|----------|--------|
| **Nom complet** | Stanford Cars Dataset |
| **Source** | Stanford AI Lab (Stanford University) |
| **Publication** | 3D Object Representations for Fine-Grained Categorization, 2013 |
| **Auteurs** | Krause, Jonathan; Stark, Michael; Deng, Jia; Fei-Fei, Li |
| **Images totales** | 16,185 images |
| **Classes originales** | 196 (marques/mod√®les : Tesla Model S 2012, BMW M3 coupe, etc.) |
| **Split original** | 8,144 train / 8,041 test |
| **Taille** | ~2 GB |
| **Format** | JPEG, RGB |

#### Structure du Dataset Stanford Cars (√† t√©l√©charger)

```
stanford_cars/
‚îÇ
‚îú‚îÄ‚îÄ üìÅ cars_train/                    # ‚úÖ √Ä UTILISER - 8,144 images
‚îÇ   ‚îú‚îÄ‚îÄ 00001.jpg
‚îÇ   ‚îú‚îÄ‚îÄ 00002.jpg
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ
‚îú‚îÄ‚îÄ üìÅ cars_test/                     # ‚úÖ √Ä UTILISER - 8,041 images
‚îÇ   ‚îú‚îÄ‚îÄ 00001.jpg
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ
‚îú‚îÄ‚îÄ üìÑ cars_train_annos.mat           # ‚ùå Ignorer (labels marques/mod√®les)
‚îú‚îÄ‚îÄ üìÑ cars_test_annos.mat            # ‚ùå Ignorer (labels marques/mod√®les)
‚îú‚îÄ‚îÄ üìÑ cars_meta.mat                  # ‚ùå Ignorer (m√©tadonn√©es des 196 classes)
‚îî‚îÄ‚îÄ üìÑ devkit/                        # ‚ùå Ignorer (outils de d√©veloppement)
```

#### Distribution des Images Stanford Cars

| Dossier | Nombre d'images | Utilisation |
|---------|-----------------|-------------|
| **cars_train/** | 8,144 | ‚úÖ Source pour √©chantillonnage |
| **cars_test/** | 8,041 | ‚úÖ Source pour √©chantillonnage |
| **TOTAL disponible** | **16,185** | Pool total |
| **TOTAL √† utiliser** | **4,000** | √âchantillon al√©atoire (seed=42) |

#### Ce qu'on utilise vs ce qu'on ignore

| √âl√©ment | Taille estim√©e | Utilisation |
|---------|----------------|-------------|
| `cars_train/` | ~1 GB | ‚úÖ **UTILISER** (images uniquement) |
| `cars_test/` | ~1 GB | ‚úÖ **UTILISER** (images uniquement) |
| `*.mat files` | ~10 MB | ‚ùå Ignorer (annotations marques/mod√®les) |
| `devkit/` | ~1 MB | ‚ùå Ignorer (scripts MATLAB) |

> **Note** : Les fichiers `.mat` contiennent les labels des 196 classes (marques et mod√®les de voitures). Pour notre projet, nous ignorons ces labels car **toutes les images Stanford Cars = classe "undamaged"**. Nous √©chantillonnons al√©atoirement 4,000 images pour √©quilibrer avec CarDD.

#### Processus d'√©chantillonnage

```python
# Pseudo-code pour l'√©chantillonnage
import random

# Charger toutes les images Stanford
all_stanford = list(cars_train/*.jpg) + list(cars_test/*.jpg)  # 16,185 images

# √âchantillonner 4,000 pour √©quilibrer avec CarDD
random.seed(42)  # Reproductibilit√©
undamaged_images = random.sample(all_stanford, k=4000)

# Toutes labellis√©es "undamaged"
```

#### Liens de R√©f√©rence

| Ressource | URL |
|-----------|-----|
| **Site officiel** | https://ai.stanford.edu/~jkrause/cars/car_dataset.html |
| **Kaggle (par classes)** | https://www.kaggle.com/datasets/jutrera/stanford-car-dataset-by-classes-folder |
| **Kaggle (full)** | https://www.kaggle.com/datasets/hassiahk/stanford-cars-dataset-full |
| **‚≠ê Kaggle (224√ó224)** | https://www.kaggle.com/datasets/jutrera/stanford-car-dataset-images-in-224x224 |
| **TensorFlow Datasets** | https://www.tensorflow.org/datasets/catalog/cars196 |

#### ‚≠ê Source Recommand√©e : Kaggle 224√ó224

> **T√©l√©charger depuis** : https://www.kaggle.com/datasets/jutrera/stanford-car-dataset-images-in-224x224

**Pourquoi cette version ?**

| Raison | Explication |
|--------|-------------|
| **Taille optimale** | Images d√©j√† redimensionn√©es en 224√ó224 pixels ‚Äî exactement la taille d'entr√©e de nos CNNs |
| **Gain de temps** | √âvite le preprocessing de ~16,000 images (redimensionnement co√ªteux en temps) |
| **Coh√©rence garantie** | Toutes les images ont strictement la m√™me dimension, pas de surprises |
| **Fichier plus l√©ger** | T√©l√©chargement plus rapide que la version full (~500 MB vs ~2 GB) |
| **Compatibilit√© PyTorch** | Pr√™t √† √™tre charg√© directement dans un DataLoader sans transformation de resize |

**Note** : La version "par classes" organise les images en 196 sous-dossiers (un par marque/mod√®le), ce qui est inutile pour nous car nous ignorons les marques ‚Äî toutes les images deviennent simplement "undamaged".

#### Citation BibTeX

```bibtex
@inproceedings{KrauseStarkDengFei-Fei_3DRR2013,
    title={3D Object Representations for Fine-Grained Categorization},
    booktitle={4th International IEEE Workshop on 3D Representation and Recognition (3dRR-13)},
    year={2013},
    address={Sydney, Australia},
    author={Jonathan Krause and Michael Stark and Jia Deng and Li Fei-Fei}
}
```

#### Utilisation dans notre projet

- **R√¥le** : Fournir la classe `UNDAMAGED`
- **S√©lection** : √âchantillon al√©atoire de 4,000 images (sur 16,185)
- **Label assign√©** : `0` (undamaged)
- **Raison de l'√©chantillonnage** : √âquilibrer les classes (50/50)

---

### 4.4 Dataset Combin√© Final

#### Vue d'ensemble

| M√©trique | Valeur |
|----------|--------|
| **Total images** | 8,000 |
| **Classe `damaged`** | 4,000 (100% de CarDD) |
| **Classe `undamaged`** | 4,000 (√©chantillon de Stanford Cars) |
| **Ratio des classes** | 50% / 50% (√©quilibr√©) |
| **Taille estim√©e** | ~7 GB |

#### Justification du Choix

| Crit√®re | √âvaluation |
|---------|------------|
| **Qualit√© acad√©mique** | ‚úÖ Deux datasets publi√©s et reconnus internationalement |
| **√âquilibre des classes** | ‚úÖ 50/50 √©vite le biais de classification |
| **Haute r√©solution** | ‚úÖ Les deux datasets offrent des images de qualit√© |
| **Diversit√©** | ‚úÖ Vari√©t√© de marques, mod√®les, angles, conditions |
| **Reproductibilit√©** | ‚úÖ Datasets publics avec liens stables |

---

### 4.5 Structure des Donn√©es (Google Drive)

> **Important** : Les donn√©es sont stock√©es sur Google Drive pour √™tre accessibles depuis Google Colab. Le code source reste en local (VS Code).

#### Architecture Hybride : Code Local + Donn√©es Cloud

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     ARCHITECTURE DU PROJET                              ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                         ‚îÇ
‚îÇ   üíª LOCAL (VS Code)                   ‚òÅÔ∏è GOOGLE DRIVE                  ‚îÇ
‚îÇ   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                ‚îÇ
‚îÇ                                                                         ‚îÇ
‚îÇ   vehicle-damage-detection/            My Drive/ENSA_Deep_Learning/     ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ src/                             ‚îú‚îÄ‚îÄ datasets/                    ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ notebooks/                       ‚îÇ   ‚îú‚îÄ‚îÄ raw/                     ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ configs/                         ‚îÇ   ‚îî‚îÄ‚îÄ processed/               ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ scripts/                         ‚îú‚îÄ‚îÄ checkpoints/                 ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ ...                              ‚îî‚îÄ‚îÄ outputs/                     ‚îÇ
‚îÇ                                                                         ‚îÇ
‚îÇ   ‚úÖ Code versionn√© (Git)              ‚úÖ Donn√©es persistantes          ‚îÇ
‚îÇ   ‚úÖ Ex√©cut√© sur Colab                 ‚úÖ Accessibles depuis Colab      ‚îÇ
‚îÇ                                                                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

#### Structure Google Drive Compl√®te

```
üìÅ My Drive/
‚îÇ
‚îî‚îÄ‚îÄ üìÅ ENSA_Deep_Learning/                        # Dossier projet principal
    ‚îÇ
    ‚îú‚îÄ‚îÄ üìÅ datasets/                              # Toutes les donn√©es
    ‚îÇ   ‚îÇ
    ‚îÇ   ‚îú‚îÄ‚îÄ üìÅ raw/                               # Donn√©es brutes t√©l√©charg√©es
    ‚îÇ   ‚îÇ   ‚îÇ
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÅ CarDD_release/                 # Dataset CarDD (~5 GB)
    ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÅ CarDD_COCO/                # ‚úÖ FORMAT √Ä UTILISER
    ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÅ annotations/           # ‚ùå Ignorer
    ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÅ train2017/             # ‚úÖ 2,816 images ‚Üí damaged
    ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÅ val2017/               # ‚úÖ 810 images ‚Üí damaged
    ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÅ test2017/              # ‚úÖ ~374 images ‚Üí damaged
    ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ
    ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÅ CarDD_SOD/                 # ‚ùå IGNORER ENTI√àREMENT
    ‚îÇ   ‚îÇ   ‚îÇ
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÅ stanford_cars_224/             # Dataset Stanford (~500 MB)
    ‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ üìÅ car_data/
    ‚îÇ   ‚îÇ           ‚îú‚îÄ‚îÄ üìÅ train/                 # ~8,144 images (196 sous-dossiers)
    ‚îÇ   ‚îÇ           ‚îî‚îÄ‚îÄ üìÅ test/                  # ~8,041 images (196 sous-dossiers)
    ‚îÇ   ‚îÇ
    ‚îÇ   ‚îî‚îÄ‚îÄ üìÅ processed/                         # Dataset combin√© (g√©n√©r√© par script)
    ‚îÇ       ‚îú‚îÄ‚îÄ üìÅ train/                         # 70% = 5,600 images
    ‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ üìÅ damaged/                   # 2,800 images
    ‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ üìÅ undamaged/                 # 2,800 images
    ‚îÇ       ‚îÇ
    ‚îÇ       ‚îú‚îÄ‚îÄ üìÅ val/                           # 15% = 1,200 images
    ‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ üìÅ damaged/                   # 600 images
    ‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ üìÅ undamaged/                 # 600 images
    ‚îÇ       ‚îÇ
    ‚îÇ       ‚îî‚îÄ‚îÄ üìÅ test/                          # 15% = 1,200 images
    ‚îÇ           ‚îú‚îÄ‚îÄ üìÅ damaged/                   # 600 images
    ‚îÇ           ‚îî‚îÄ‚îÄ üìÅ undamaged/                 # 600 images
    ‚îÇ
    ‚îú‚îÄ‚îÄ üìÅ checkpoints/                           # Mod√®les sauvegard√©s (persistants)
    ‚îÇ   ‚îú‚îÄ‚îÄ üìÅ model_a/                           # Checkpoints Model A (VGG-like)
    ‚îÇ   ‚îî‚îÄ‚îÄ üìÅ model_b/                           # Checkpoints Model B (Skip connections)
    ‚îÇ
    ‚îî‚îÄ‚îÄ üìÅ outputs/                               # R√©sultats et logs
        ‚îú‚îÄ‚îÄ üìÅ figures/                           # Graphiques, courbes d'apprentissage
        ‚îî‚îÄ‚îÄ üìÅ logs/                              # TensorBoard logs
```

#### Chemins d'Acc√®s depuis Colab

| Ressource | Chemin Colab |
|-----------|--------------|
| **Racine Drive** | `/content/drive/MyDrive/` |
| **Projet** | `/content/drive/MyDrive/ENSA_Deep_Learning/` |
| **Datasets raw** | `/content/drive/MyDrive/ENSA_Deep_Learning/datasets/raw/` |
| **Datasets processed** | `/content/drive/MyDrive/ENSA_Deep_Learning/datasets/processed/` |
| **CarDD images** | `/content/drive/MyDrive/ENSA_Deep_Learning/datasets/raw/CarDD_release/CarDD_COCO/` |
| **Stanford images** | `/content/drive/MyDrive/ENSA_Deep_Learning/datasets/raw/stanford_cars_224/car_data/` |
| **Checkpoints** | `/content/drive/MyDrive/ENSA_Deep_Learning/checkpoints/` |
| **Outputs** | `/content/drive/MyDrive/ENSA_Deep_Learning/outputs/` |

#### Note sur le Preprocessing

Le script de pr√©paration des donn√©es (ex√©cut√© dans Colab) devra :
1. **Monter** Google Drive avec `drive.mount('/content/drive')`
2. **Collecter** les images de `CarDD_COCO/train2017/`, `val2017/`, `test2017/` ‚Üí toutes = `damaged`
3. **Collecter** les images de `stanford_cars_224/car_data/train/` et `test/` (tous les sous-dossiers) ‚Üí toutes = `undamaged`
4. **√âchantillonner** 4,000 images de Stanford pour √©quilibrer avec CarDD (seed=42)
5. **M√©langer** et **splitter** en 70/15/15 de mani√®re stratifi√©e
6. **Copier** les images dans la structure `datasets/processed/`

---

### 4.6 Pr√©traitement des Images

```python
# Configuration du pr√©traitement
PREPROCESSING_CONFIG = {
    "input_size": (224, 224),           # Taille d'entr√©e standard CNN
    "normalization": {
        "mean": [0.485, 0.456, 0.406],  # Statistiques ImageNet (r√©f√©rence)
        "std": [0.229, 0.224, 0.225]
    },
    "color_space": "RGB"
}
```

#### Pipeline de Pr√©traitement

1. **Chargement** : Lecture de l'image (PIL ou OpenCV)
2. **Redimensionnement** : Resize to 224√ó224 pixels
3. **Normalisation** : Scale [0, 255] ‚Üí [0, 1] puis normalisation mean/std
4. **Conversion** : PIL Image ‚Üí Tensor PyTorch (C, H, W)

---

### 4.7 Augmentation des Donn√©es

```python
# Configuration d'augmentation pour l'entra√Ænement
TRAIN_AUGMENTATION = {
    "RandomHorizontalFlip": {"p": 0.5},
    "RandomRotation": {"degrees": 15},
    "ColorJitter": {
        "brightness": 0.2,
        "contrast": 0.2,
        "saturation": 0.1,
        "hue": 0.05
    },
    "RandomResizedCrop": {
        "size": 224,
        "scale": (0.8, 1.0)
    }
}

# Pas d'augmentation pour validation/test
VAL_TEST_AUGMENTATION = None
```

#### Justification des Augmentations

| Augmentation | Justification |
|--------------|---------------|
| `HorizontalFlip` | Les dommages peuvent appara√Ætre √† gauche ou √† droite du v√©hicule |
| `Rotation (¬±15¬∞)` | Simule les diff√©rents angles de prise de vue lors de l'inspection |
| `ColorJitter` | Compense les variations d'√©clairage (int√©rieur, ext√©rieur, nuit) |
| `RandomResizedCrop` | Simule diff√©rentes distances entre la cam√©ra et le v√©hicule |

---

### 4.8 Split des Donn√©es

```python
DATA_SPLIT = {
    "train": 0.70,      # 70% pour l'entra√Ænement = 5,600 images
    "val": 0.15,        # 15% pour la validation = 1,200 images
    "test": 0.15,       # 15% pour le test final = 1,200 images
    "random_seed": 42,  # Pour reproductibilit√©
    "stratified": True  # Maintenir le ratio 50/50 dans chaque split
}
```

#### Distribution Finale

| Split | Total | Damaged | Undamaged |
|-------|-------|---------|-----------|
| **Train** | 5,600 | 2,800 | 2,800 |
| **Validation** | 1,200 | 600 | 600 |
| **Test** | 1,200 | 600 | 600 |
| **TOTAL** | **8,000** | **4,000** | **4,000** |

---

## 5. Architecture Model A ‚Äî Baseline VGG-like

### 5.1 Philosophie de Conception

> **Principe VGG** : Utiliser des convolutions 3√ó3 empil√©es plut√¥t que de grands kernels.  
> **Avantage** : M√™me champ r√©ceptif avec moins de param√®tres et plus de non-lin√©arit√©s.

**Pourquoi VGG-like pour la baseline ?**
- Architecture simple et bien comprise
- Facile √† impl√©menter et d√©bugger
- Bon point de r√©f√©rence pour mesurer l'apport des skip connections

### 5.2 Sp√©cifications Architecturales

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    MODEL A - BASELINE CNN                    ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                             ‚îÇ
‚îÇ  Input: (batch, 3, 224, 224)                               ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ BLOCK 1                                              ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ Conv2d(3‚Üí32, k=3, p=1) ‚Üí ReLU                       ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ Conv2d(32‚Üí32, k=3, p=1) ‚Üí ReLU                      ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ MaxPool2d(2, 2)                                      ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ Output: (batch, 32, 112, 112)                        ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                          ‚Üì                                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ BLOCK 2                                              ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ Conv2d(32‚Üí64, k=3, p=1) ‚Üí ReLU                      ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ Conv2d(64‚Üí64, k=3, p=1) ‚Üí ReLU                      ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ MaxPool2d(2, 2)                                      ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ Output: (batch, 64, 56, 56)                          ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                          ‚Üì                                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ BLOCK 3                                              ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ Conv2d(64‚Üí128, k=3, p=1) ‚Üí ReLU                     ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ Conv2d(128‚Üí128, k=3, p=1) ‚Üí ReLU                    ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ MaxPool2d(2, 2)                                      ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ Output: (batch, 128, 28, 28)                         ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                          ‚Üì                                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ BLOCK 4                                              ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ Conv2d(128‚Üí256, k=3, p=1) ‚Üí ReLU                    ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ Conv2d(256‚Üí256, k=3, p=1) ‚Üí ReLU                    ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ MaxPool2d(2, 2)                                      ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ Output: (batch, 256, 14, 14)                         ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                          ‚Üì                                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ BLOCK 5                                              ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ Conv2d(256‚Üí512, k=3, p=1) ‚Üí ReLU                    ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ Conv2d(512‚Üí512, k=3, p=1) ‚Üí ReLU                    ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ MaxPool2d(2, 2)                                      ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ Output: (batch, 512, 7, 7)                           ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                          ‚Üì                                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ CLASSIFIER                                           ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ AdaptiveAvgPool2d(1, 1) ‚Üí Flatten                   ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ Linear(512‚Üí256) ‚Üí ReLU ‚Üí Dropout(0.5)               ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ Linear(256‚Üínum_classes)                              ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ Output: (batch, num_classes)                         ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                                                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 5.3 Code de R√©f√©rence

```python
import torch
import torch.nn as nn

class VGGBlock(nn.Module):
    """
    Bloc VGG-style : deux convolutions 3√ó3 suivies de MaxPool.
    
    Justification architecturale:
    - Deux conv 3√ó3 = champ r√©ceptif √©quivalent √† une conv 5√ó5
    - Mais avec moins de param√®tres (2√ó3¬≤√óC¬≤ vs 5¬≤√óC¬≤)
    - Et une non-lin√©arit√© suppl√©mentaire (meilleure capacit√© d'apprentissage)
    """
    def __init__(self, in_channels: int, out_channels: int):
        super().__init__()
        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)
        self.relu = nn.ReLU(inplace=True)
        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = self.relu(self.conv1(x))
        x = self.relu(self.conv2(x))
        x = self.pool(x)
        return x


class BaselineCNN(nn.Module):
    """
    Architecture CNN Baseline inspir√©e de VGG.
    
    Caract√©ristiques:
    - 5 blocs convolutifs avec doublement progressif des canaux
    - Adaptive pooling pour flexibilit√© de taille d'entr√©e
    - Classifier avec dropout pour r√©gularisation
    
    Param√®tres totaux estim√©s: ~6.5M
    """
    def __init__(self, num_classes: int = 2, dropout_rate: float = 0.5):
        super().__init__()
        
        # Feature extractor
        self.features = nn.Sequential(
            VGGBlock(3, 32),      # 224‚Üí112, 3‚Üí32 canaux
            VGGBlock(32, 64),     # 112‚Üí56, 32‚Üí64 canaux
            VGGBlock(64, 128),    # 56‚Üí28, 64‚Üí128 canaux
            VGGBlock(128, 256),   # 28‚Üí14, 128‚Üí256 canaux
            VGGBlock(256, 512),   # 14‚Üí7, 256‚Üí512 canaux
        )
        
        # Global pooling + Classifier
        self.classifier = nn.Sequential(
            nn.AdaptiveAvgPool2d((1, 1)),  # 7√ó7‚Üí1√ó1
            nn.Flatten(),
            nn.Linear(512, 256),
            nn.ReLU(inplace=True),
            nn.Dropout(p=dropout_rate),
            nn.Linear(256, num_classes)
        )
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = self.features(x)
        x = self.classifier(x)
        return x
    
    def get_num_parameters(self) -> int:
        """Retourne le nombre total de param√®tres."""
        return sum(p.numel() for p in self.parameters())
    
    def get_num_trainable_parameters(self) -> int:
        """Retourne le nombre de param√®tres entra√Ænables."""
        return sum(p.numel() for p in self.parameters() if p.requires_grad)
```

### 5.4 Justification des Choix Architecturaux

| Choix | Pourquoi | Alternative Consid√©r√©e |
|-------|----------|------------------------|
| Convolutions 3√ó3 | Petit kernel mais champ r√©ceptif efficace via empilement | 5√ó5 ou 7√ó7 (plus de param√®tres) |
| Doublement des canaux | Convention standard, capture features de plus en plus abstraites | Croissance lin√©aire |
| MaxPool 2√ó2 | R√©duction spatiale efficace, invariance locale | AvgPool (moins discriminant) |
| AdaptiveAvgPool | Flexibilit√© taille d'entr√©e, r√©duit overfitting vs FC large | Flatten direct (trop de params) |
| Dropout 0.5 | R√©gularisation standard pour classifier | Dropout plus faible (moins efficace) |
| ReLU | Simple, efficace, pas de vanishing gradient | LeakyReLU (pas n√©cessaire ici) |

### 5.5 Analyse du Champ R√©ceptif

```
Couche          Champ R√©ceptif    Explication
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Input           1√ó1               Pixel initial
Block1-Conv1    3√ó3               Premier kernel
Block1-Conv2    5√ó5               3 + (3-1) = 5
Block1-Pool     6√ó6               5 + 1 = 6 (stride 2)
Block2-Conv1    10√ó10             6√ó2 + (3-1) = 14? Non: (6-1)√ó2 + 3
...
Block5-Pool     ~180√ó180          Couvre une grande partie de l'image 224√ó224
```

**Conclusion** : Le champ r√©ceptif final permet de capturer des patterns √† l'√©chelle de dommages typiques (quelques cm sur une voiture ‚âà 50-150 pixels sur une image 224√ó224).

---

## 6. Architecture Model B ‚Äî Deep CNN avec Skip Connections

### 6.1 Philosophie de Conception

> **Principe ResNet** : Les connexions r√©siduelles permettent d'entra√Æner des r√©seaux plus profonds en facilitant le flux de gradients.

**Formulation math√©matique** :
```
Output = F(x) + x       (skip connection)
```
Au lieu d'apprendre `H(x)`, le r√©seau apprend `F(x) = H(x) - x` (le r√©sidu).

**Pourquoi les skip connections ?**
- Att√©nuent le probl√®me de vanishing gradient
- Permettent l'entra√Ænement de r√©seaux plus profonds
- L'identit√© est facile √† apprendre si n√©cessaire (F(x) ‚Üí 0)

### 6.2 Sp√©cifications Architecturales

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                MODEL B - DEEP CNN WITH SKIP CONNECTIONS      ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                             ‚îÇ
‚îÇ  Input: (batch, 3, 224, 224)                               ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ STEM                                                 ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ Conv2d(3‚Üí64, k=7, s=2, p=3) ‚Üí BN ‚Üí ReLU             ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ MaxPool2d(3, 2, 1)                                   ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ Output: (batch, 64, 56, 56)                          ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                          ‚Üì                                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ STAGE 1: 2√ó ResidualBlock(64‚Üí64)                    ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ ‚îÇ  x ‚îÄ‚îÄ‚Üí Conv‚ÜíBN‚ÜíReLU‚ÜíConv‚ÜíBN ‚îÄ‚îÄ‚Üí (+) ‚Üí ReLU   ‚îÇ   ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üó             ‚îÇ   ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ Output: (batch, 64, 56, 56)                          ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                          ‚Üì                                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ STAGE 2: 2√ó ResidualBlock(64‚Üí128), stride=2 first   ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ Output: (batch, 128, 28, 28)                         ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                          ‚Üì                                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ STAGE 3: 2√ó ResidualBlock(128‚Üí256), stride=2 first  ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ Output: (batch, 256, 14, 14)                         ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                          ‚Üì                                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ STAGE 4: 2√ó ResidualBlock(256‚Üí512), stride=2 first  ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ Output: (batch, 512, 7, 7)                           ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                          ‚Üì                                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ HEAD                                                 ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ AdaptiveAvgPool2d(1, 1) ‚Üí Flatten                   ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ Linear(512‚Üínum_classes)                              ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ Output: (batch, num_classes)                         ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                                                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 6.3 Code de R√©f√©rence

```python
import torch
import torch.nn as nn


class ResidualBlock(nn.Module):
    """
    Bloc r√©siduel basique avec skip connection.
    
    Architecture:
        x ‚Üí Conv ‚Üí BN ‚Üí ReLU ‚Üí Conv ‚Üí BN ‚Üí (+) ‚Üí ReLU
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üó
    
    Si downsample=True ou changement de canaux:
        La skip connection passe par une conv 1√ó1 pour matcher les dimensions.
    
    Justification:
    - Skip connection permet au gradient de "bypass" les convolutions
    - BatchNorm stabilise l'entra√Ænement et acc√©l√®re la convergence
    - Placement BN apr√®s Conv (style original ResNet)
    """
    def __init__(
        self, 
        in_channels: int, 
        out_channels: int, 
        stride: int = 1,
        downsample: nn.Module = None
    ):
        super().__init__()
        
        # Premi√®re convolution (peut r√©duire la taille spatiale)
        self.conv1 = nn.Conv2d(
            in_channels, out_channels, 
            kernel_size=3, stride=stride, padding=1, bias=False
        )
        self.bn1 = nn.BatchNorm2d(out_channels)
        
        # Deuxi√®me convolution (maintient la taille)
        self.conv2 = nn.Conv2d(
            out_channels, out_channels,
            kernel_size=3, stride=1, padding=1, bias=False
        )
        self.bn2 = nn.BatchNorm2d(out_channels)
        
        self.relu = nn.ReLU(inplace=True)
        self.downsample = downsample
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        identity = x
        
        # Branche principale
        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)
        
        out = self.conv2(out)
        out = self.bn2(out)
        
        # Skip connection (avec projection si n√©cessaire)
        if self.downsample is not None:
            identity = self.downsample(x)
        
        # Addition et activation finale
        out += identity
        out = self.relu(out)
        
        return out


class DeepCNN(nn.Module):
    """
    Architecture Deep CNN avec skip connections inspir√©e de ResNet.
    
    Caract√©ristiques:
    - Stem agressif (conv 7√ó7 stride 2 + maxpool) pour r√©duction rapide
    - 4 stages avec blocs r√©siduels
    - Global Average Pooling pour r√©duire l'overfitting
    - Classifier minimaliste (une seule couche FC)
    
    Diff√©rences cl√©s avec Model A:
    - Skip connections pour meilleur flux de gradient
    - BatchNorm pour stabilit√©
    - Plus profond (18 couches conv vs 10)
    - Moins de param√®tres dans le classifier
    
    Param√®tres totaux estim√©s: ~11M
    """
    def __init__(self, num_classes: int = 2):
        super().__init__()
        
        # Stem: r√©duction rapide de la r√©solution
        self.stem = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
        )
        
        # Stages de blocs r√©siduels
        self.stage1 = self._make_stage(64, 64, num_blocks=2, stride=1)
        self.stage2 = self._make_stage(64, 128, num_blocks=2, stride=2)
        self.stage3 = self._make_stage(128, 256, num_blocks=2, stride=2)
        self.stage4 = self._make_stage(256, 512, num_blocks=2, stride=2)
        
        # Classification head
        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(512, num_classes)
        
        # Initialisation des poids
        self._initialize_weights()
    
    def _make_stage(
        self, 
        in_channels: int, 
        out_channels: int, 
        num_blocks: int, 
        stride: int
    ) -> nn.Sequential:
        """
        Cr√©e un stage compos√© de plusieurs blocs r√©siduels.
        
        Le premier bloc peut avoir un stride > 1 pour downsampling.
        Les blocs suivants maintiennent la r√©solution.
        """
        downsample = None
        
        # Projection si changement de dimensions
        if stride != 1 or in_channels != out_channels:
            downsample = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, kernel_size=1, 
                         stride=stride, bias=False),
                nn.BatchNorm2d(out_channels)
            )
        
        layers = []
        
        # Premier bloc (peut downsample)
        layers.append(ResidualBlock(in_channels, out_channels, stride, downsample))
        
        # Blocs suivants (maintiennent la r√©solution)
        for _ in range(1, num_blocks):
            layers.append(ResidualBlock(out_channels, out_channels))
        
        return nn.Sequential(*layers)
    
    def _initialize_weights(self):
        """
        Initialisation des poids selon les bonnes pratiques.
        
        - Conv: Kaiming He (adapt√© pour ReLU)
        - BatchNorm: weight=1, bias=0
        - Linear: Normal avec petit std
        """
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.Linear):
                nn.init.normal_(m.weight, 0, 0.01)
                nn.init.constant_(m.bias, 0)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # Stem
        x = self.stem(x)
        
        # Stages
        x = self.stage1(x)
        x = self.stage2(x)
        x = self.stage3(x)
        x = self.stage4(x)
        
        # Classification
        x = self.avgpool(x)
        x = torch.flatten(x, 1)
        x = self.fc(x)
        
        return x
    
    def get_num_parameters(self) -> int:
        """Retourne le nombre total de param√®tres."""
        return sum(p.numel() for p in self.parameters())
```

### 6.4 Justification des Choix Architecturaux

| Choix | Pourquoi | Impact Attendu |
|-------|----------|----------------|
| Skip connections | Flux de gradient am√©lior√©, entra√Ænement stable | Convergence plus rapide, possibilit√© d'aller plus profond |
| BatchNorm | Normalisation des activations, r√©gularisation implicite | Stabilit√©, acc√©l√©ration |
| Stem 7√ó7 stride 2 | R√©duction rapide de la r√©solution d√®s le d√©but | Moins de compute dans les stages suivants |
| Conv 1√ó1 pour projection | Matcher les dimensions avec minimum de param√®tres | Skip connection fonctionnelle m√™me avec changement de taille |
| Global Average Pool | R√©duction drastique des param√®tres | Moins d'overfitting que FC large |
| Pas de Dropout | BatchNorm fournit d√©j√† une r√©gularisation | Simplicit√© |

### 6.5 Comparaison Model A vs Model B

| Aspect | Model A (Baseline) | Model B (Deep) |
|--------|-------------------|----------------|
| Profondeur (couches conv) | 10 | 18 |
| Skip connections | ‚ùå Non | ‚úÖ Oui |
| BatchNorm | ‚ùå Non | ‚úÖ Oui |
| Params (estim√©s) | ~6.5M | ~11M |
| R√©gularisation | Dropout 0.5 | BatchNorm |
| Complexit√© | Simple | Mod√©r√©e |
| Risque vanishing gradient | Moyen | Faible |

---

## 7. Pipeline d'Entra√Ænement

### 7.1 Configuration G√©n√©rale

```python
TRAINING_CONFIG = {
    # Hyperparam√®tres de base
    "batch_size": 32,
    "num_epochs": 100,
    "learning_rate": 1e-3,
    "weight_decay": 1e-4,
    
    # Optimiseur
    "optimizer": "Adam",
    "optimizer_params": {
        "betas": (0.9, 0.999),
        "eps": 1e-8
    },
    
    # Scheduler
    "scheduler": "ReduceLROnPlateau",
    "scheduler_params": {
        "mode": "min",
        "factor": 0.1,
        "patience": 5,
        "min_lr": 1e-6
    },
    
    # Early stopping
    "early_stopping": {
        "patience": 10,
        "min_delta": 1e-4,
        "monitor": "val_loss"
    },
    
    # Checkpointing
    "save_best_only": True,
    "checkpoint_dir": "checkpoints/",
    
    # Reproductibilit√©
    "random_seed": 42,
    "deterministic": True
}
```

### 7.2 Fonction de Perte

```python
# Pour classification binaire
criterion = nn.CrossEntropyLoss()

# Alternative si classes d√©s√©quilibr√©es
class_weights = torch.tensor([1.0, 2.0])  # Exemple: 2√ó poids pour 'damaged'
criterion = nn.CrossEntropyLoss(weight=class_weights)
```

**Justification** : CrossEntropyLoss combine LogSoftmax et NLLLoss, adapt√© √† la classification multi-classes (m√™me binaire avec 2 classes).

### 7.3 Boucle d'Entra√Ænement (Pseudo-code)

```python
def train_one_epoch(model, dataloader, criterion, optimizer, device):
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0
    
    for batch_idx, (inputs, targets) in enumerate(dataloader):
        inputs, targets = inputs.to(device), targets.to(device)
        
        # Forward pass
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, targets)
        
        # Backward pass
        loss.backward()
        optimizer.step()
        
        # M√©triques
        running_loss += loss.item()
        _, predicted = outputs.max(1)
        total += targets.size(0)
        correct += predicted.eq(targets).sum().item()
    
    epoch_loss = running_loss / len(dataloader)
    epoch_acc = 100. * correct / total
    
    return epoch_loss, epoch_acc


def validate(model, dataloader, criterion, device):
    model.eval()
    running_loss = 0.0
    correct = 0
    total = 0
    all_preds = []
    all_targets = []
    
    with torch.no_grad():
        for inputs, targets in dataloader:
            inputs, targets = inputs.to(device), targets.to(device)
            
            outputs = model(inputs)
            loss = criterion(outputs, targets)
            
            running_loss += loss.item()
            _, predicted = outputs.max(1)
            total += targets.size(0)
            correct += predicted.eq(targets).sum().item()
            
            all_preds.extend(predicted.cpu().numpy())
            all_targets.extend(targets.cpu().numpy())
    
    epoch_loss = running_loss / len(dataloader)
    epoch_acc = 100. * correct / total
    
    return epoch_loss, epoch_acc, all_preds, all_targets
```

### 7.4 Early Stopping

```python
class EarlyStopping:
    """
    Arr√™te l'entra√Ænement si la m√©trique ne s'am√©liore pas.
    
    Justification:
    - √âvite l'overfitting en stoppant au bon moment
    - √âconomise du temps de calcul
    - S√©lectionne automatiquement le meilleur mod√®le
    """
    def __init__(self, patience: int = 10, min_delta: float = 1e-4):
        self.patience = patience
        self.min_delta = min_delta
        self.counter = 0
        self.best_loss = None
        self.early_stop = False
    
    def __call__(self, val_loss: float) -> bool:
        if self.best_loss is None:
            self.best_loss = val_loss
        elif val_loss > self.best_loss - self.min_delta:
            self.counter += 1
            if self.counter >= self.patience:
                self.early_stop = True
        else:
            self.best_loss = val_loss
            self.counter = 0
        
        return self.early_stop
```

---

## 8. Protocole d'√âvaluation

### 8.1 M√©triques Principales

| M√©trique | Formule | Interpr√©tation |
|----------|---------|----------------|
| **Accuracy** | (TP + TN) / Total | Performance globale |
| **Precision** | TP / (TP + FP) | "Quand je pr√©dis damaged, ai-je raison ?" |
| **Recall** | TP / (TP + FN) | "Est-ce que je d√©tecte tous les dommages ?" |
| **F1-Score** | 2 √ó (P √ó R) / (P + R) | Compromis precision/recall |

**O√π** : TP = True Positive (damaged pr√©dit et r√©el), FP = False Positive, etc.

### 8.2 Code d'√âvaluation

```python
from sklearn.metrics import (
    accuracy_score, 
    precision_score, 
    recall_score, 
    f1_score,
    confusion_matrix,
    classification_report
)
import matplotlib.pyplot as plt
import seaborn as sns


def evaluate_model(y_true, y_pred, class_names=['undamaged', 'damaged']):
    """
    √âvaluation compl√®te d'un mod√®le de classification.
    
    Retourne:
    - Dictionnaire de m√©triques
    - Matrice de confusion
    - Rapport de classification
    """
    metrics = {
        'accuracy': accuracy_score(y_true, y_pred),
        'precision': precision_score(y_true, y_pred, average='weighted'),
        'recall': recall_score(y_true, y_pred, average='weighted'),
        'f1_score': f1_score(y_true, y_pred, average='weighted'),
        'precision_per_class': precision_score(y_true, y_pred, average=None),
        'recall_per_class': recall_score(y_true, y_pred, average=None),
        'f1_per_class': f1_score(y_true, y_pred, average=None)
    }
    
    cm = confusion_matrix(y_true, y_pred)
    report = classification_report(y_true, y_pred, target_names=class_names)
    
    return metrics, cm, report


def plot_confusion_matrix(cm, class_names, title='Confusion Matrix'):
    """Visualise la matrice de confusion."""
    plt.figure(figsize=(8, 6))
    sns.heatmap(
        cm, 
        annot=True, 
        fmt='d', 
        cmap='Blues',
        xticklabels=class_names,
        yticklabels=class_names
    )
    plt.title(title)
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    plt.tight_layout()
    return plt.gcf()


def plot_training_history(history: dict, save_path: str = None):
    """
    Visualise l'historique d'entra√Ænement.
    
    Args:
        history: {'train_loss': [...], 'val_loss': [...], 
                  'train_acc': [...], 'val_acc': [...]}
    """
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))
    
    # Loss
    axes[0].plot(history['train_loss'], label='Train Loss')
    axes[0].plot(history['val_loss'], label='Val Loss')
    axes[0].set_xlabel('Epoch')
    axes[0].set_ylabel('Loss')
    axes[0].set_title('Training vs Validation Loss')
    axes[0].legend()
    axes[0].grid(True)
    
    # Accuracy
    axes[1].plot(history['train_acc'], label='Train Acc')
    axes[1].plot(history['val_acc'], label='Val Acc')
    axes[1].set_xlabel('Epoch')
    axes[1].set_ylabel('Accuracy (%)')
    axes[1].set_title('Training vs Validation Accuracy')
    axes[1].legend()
    axes[1].grid(True)
    
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
    
    return fig
```

### 8.3 Comparaison des Mod√®les

```python
def compare_models(results_a: dict, results_b: dict):
    """
    Compare les performances de Model A et Model B.
    
    Args:
        results_a: M√©triques du Model A
        results_b: M√©triques du Model B
    
    Returns:
        DataFrame de comparaison
    """
    import pandas as pd
    
    comparison = pd.DataFrame({
        'Metric': ['Accuracy', 'Precision', 'Recall', 'F1-Score'],
        'Model A (Baseline)': [
            results_a['accuracy'],
            results_a['precision'],
            results_a['recall'],
            results_a['f1_score']
        ],
        'Model B (Deep)': [
            results_b['accuracy'],
            results_b['precision'],
            results_b['recall'],
            results_b['f1_score']
        ]
    })
    
    comparison['Œî (B - A)'] = comparison['Model B (Deep)'] - comparison['Model A (Baseline)']
    comparison['Improvement (%)'] = (comparison['Œî (B - A)'] / comparison['Model A (Baseline)']) * 100
    
    return comparison
```

### 8.4 Ablation Studies (Optionnel mais Recommand√©)

| Exp√©rience | Variable Modifi√©e | Objectif |
|------------|-------------------|----------|
| Ablation 1 | Model B sans BatchNorm | Mesurer l'apport de BatchNorm |
| Ablation 2 | Model B sans skip connections | V√©rifier que les skips aident |
| Ablation 3 | Model A avec BatchNorm | BatchNorm aide-t-il m√™me sans skips ? |
| Ablation 4 | Sans data augmentation | Mesurer l'apport de l'augmentation |

---

## 9. Structure du Projet

### 9.1 Arborescence Recommand√©e (Code Local)

> **Note** : Les donn√©es, checkpoints et outputs sont stock√©s sur Google Drive (voir section 4.5). Le code source est versionn√© localement avec Git.

```
vehicle-damage-detection/                # üíª LOCAL (VS Code + Git)
‚îÇ
‚îú‚îÄ‚îÄ README.md                            # Documentation principale
‚îú‚îÄ‚îÄ PRD.md                               # Ce document
‚îú‚îÄ‚îÄ requirements.txt                     # D√©pendances Python (pour r√©f√©rence)
‚îú‚îÄ‚îÄ LICENSE                              # Licence MIT
‚îú‚îÄ‚îÄ .gitignore                           # Fichiers √† ignorer
‚îÇ
‚îú‚îÄ‚îÄ configs/                             # Fichiers de configuration
‚îÇ   ‚îú‚îÄ‚îÄ config.yaml                      # Configuration principale
‚îÇ   ‚îú‚îÄ‚îÄ model_a_config.yaml              # Config sp√©cifique Model A
‚îÇ   ‚îî‚îÄ‚îÄ model_b_config.yaml              # Config sp√©cifique Model B
‚îÇ
‚îú‚îÄ‚îÄ notebooks/                           # Notebooks Jupyter (ex√©cut√©s sur Colab)
‚îÇ   ‚îú‚îÄ‚îÄ 00_setup_colab.ipynb             # Setup initial Colab + v√©rification GPU
‚îÇ   ‚îú‚îÄ‚îÄ 01_data_exploration.ipynb        # Exploration des donn√©es
‚îÇ   ‚îú‚îÄ‚îÄ 02_preprocessing.ipynb           # Cr√©ation du dataset processed/
‚îÇ   ‚îú‚îÄ‚îÄ 03_train_baseline.ipynb          # Entra√Ænement Model A
‚îÇ   ‚îú‚îÄ‚îÄ 04_train_deep.ipynb              # Entra√Ænement Model B
‚îÇ   ‚îú‚îÄ‚îÄ 05_evaluation.ipynb              # √âvaluation et comparaison
‚îÇ   ‚îî‚îÄ‚îÄ 06_analysis.ipynb                # Analyse des erreurs
‚îÇ
‚îú‚îÄ‚îÄ src/                                 # Code source (import√© dans notebooks)
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ data/                            # Gestion des donn√©es
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dataset.py                   # Classes Dataset PyTorch
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ transforms.py                # Transformations et augmentations
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ utils.py                     # Utilitaires data
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ models/                          # Architectures CNN
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ baseline_cnn.py              # Model A (VGG-like)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ deep_cnn.py                  # Model B (Skip connections)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ components.py                # Blocs r√©utilisables (VGGBlock, ResidualBlock)
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ training/                        # Entra√Ænement
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ trainer.py                   # Classe Trainer
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ callbacks.py                 # Early stopping, checkpointing
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ losses.py                    # Fonctions de perte custom
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ evaluation/                      # √âvaluation
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ metrics.py                   # Calcul des m√©triques
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ visualization.py             # Graphiques et plots
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ utils/                           # Utilitaires g√©n√©raux
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îú‚îÄ‚îÄ config.py                    # Chargement config YAML
‚îÇ       ‚îú‚îÄ‚îÄ seed.py                      # Reproductibilit√©
‚îÇ       ‚îú‚îÄ‚îÄ paths.py                     # Chemins Google Drive (NEW)
‚îÇ       ‚îî‚îÄ‚îÄ logging.py                   # Logging
‚îÇ
‚îú‚îÄ‚îÄ app/                                 # [SECONDAIRE] Application Flask
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ app.py                           # Application Flask principale
‚îÇ   ‚îú‚îÄ‚îÄ templates/                       # Templates HTML
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ index.html
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ result.html
‚îÇ   ‚îú‚îÄ‚îÄ static/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ style.css
‚îÇ   ‚îî‚îÄ‚îÄ utils/
‚îÇ       ‚îú‚îÄ‚îÄ predictor.py
‚îÇ       ‚îî‚îÄ‚îÄ report_generator.py
‚îÇ
‚îú‚îÄ‚îÄ scripts/                             # Scripts ex√©cutables
‚îÇ   ‚îú‚îÄ‚îÄ prepare_data.py                  # Script pr√©paration donn√©es
‚îÇ   ‚îú‚îÄ‚îÄ train.py                         # Script d'entra√Ænement
‚îÇ   ‚îú‚îÄ‚îÄ evaluate.py                      # Script d'√©valuation
‚îÇ   ‚îî‚îÄ‚îÄ predict.py                       # Script d'inf√©rence
‚îÇ
‚îú‚îÄ‚îÄ docs/                                # Documentation additionnelle
‚îÇ   ‚îú‚îÄ‚îÄ architecture.md
‚îÇ   ‚îú‚îÄ‚îÄ evaluation_protocol.md
‚îÇ   ‚îî‚îÄ‚îÄ report_template.md
‚îÇ
‚îú‚îÄ‚îÄ presentation/                        # Pr√©sentation PowerPoint
‚îÇ   ‚îî‚îÄ‚îÄ slides.pptx
‚îÇ
‚îî‚îÄ‚îÄ tests/                               # Tests unitaires (optionnel)
    ‚îú‚îÄ‚îÄ test_models.py
    ‚îî‚îÄ‚îÄ test_data.py
```

### 9.1.1 Structure Google Drive (Rappel)

```
üìÅ My Drive/ENSA_Deep_Learning/          # ‚òÅÔ∏è GOOGLE DRIVE
‚îÇ
‚îú‚îÄ‚îÄ üìÅ datasets/
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ raw/                          # CarDD + Stanford Cars
‚îÇ   ‚îî‚îÄ‚îÄ üìÅ processed/                    # Dataset combin√© train/val/test
‚îÇ
‚îú‚îÄ‚îÄ üìÅ checkpoints/
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ model_a/                      # Sauvegardes Model A
‚îÇ   ‚îî‚îÄ‚îÄ üìÅ model_b/                      # Sauvegardes Model B
‚îÇ
‚îî‚îÄ‚îÄ üìÅ outputs/
    ‚îú‚îÄ‚îÄ üìÅ figures/                      # Graphiques g√©n√©r√©s
    ‚îî‚îÄ‚îÄ üìÅ logs/                         # TensorBoard logs
```

### 9.1.2 Fichier paths.py (Chemins centralis√©s)

```python
# src/utils/paths.py
"""
Chemins centralis√©s pour Google Drive.
√Ä importer dans tous les notebooks et scripts.
"""

# Racine Google Drive (apr√®s mount)
DRIVE_ROOT = "/content/drive/MyDrive"

# Projet
PROJECT_ROOT = f"{DRIVE_ROOT}/ENSA_Deep_Learning"

# Datasets
DATASETS_DIR = f"{PROJECT_ROOT}/datasets"
RAW_DATA_DIR = f"{DATASETS_DIR}/raw"
PROCESSED_DATA_DIR = f"{DATASETS_DIR}/processed"

# Datasets bruts
CARDD_DIR = f"{RAW_DATA_DIR}/CarDD_release/CarDD_COCO"
STANFORD_DIR = f"{RAW_DATA_DIR}/stanford_cars_224/car_data"

# Splits processed
TRAIN_DIR = f"{PROCESSED_DATA_DIR}/train"
VAL_DIR = f"{PROCESSED_DATA_DIR}/val"
TEST_DIR = f"{PROCESSED_DATA_DIR}/test"

# Checkpoints
CHECKPOINTS_DIR = f"{PROJECT_ROOT}/checkpoints"
MODEL_A_CKPT = f"{CHECKPOINTS_DIR}/model_a"
MODEL_B_CKPT = f"{CHECKPOINTS_DIR}/model_b"

# Outputs
OUTPUTS_DIR = f"{PROJECT_ROOT}/outputs"
FIGURES_DIR = f"{OUTPUTS_DIR}/figures"
LOGS_DIR = f"{OUTPUTS_DIR}/logs"
```

### 9.2 requirements.txt

```
# Core
torch>=2.0.0
torchvision>=0.15.0
numpy>=1.24.0
pandas>=2.0.0

# Data & Preprocessing
Pillow>=9.5.0
scikit-learn>=1.3.0
albumentations>=1.3.0

# Visualization
matplotlib>=3.7.0
seaborn>=0.12.0

# Configuration
pyyaml>=6.0
python-dotenv>=1.0.0

# Notebooks
jupyter>=1.0.0
ipywidgets>=8.0.0

# Progress & Logging
tqdm>=4.65.0
tensorboard>=2.13.0

# [SECONDAIRE] Application Flask
flask>=3.0.0
werkzeug>=3.0.0

# [SECONDAIRE] G√©n√©ration de rapports PDF
reportlab>=4.0.0
fpdf2>=2.7.0

# Optional: Experiment tracking
# wandb>=0.15.0
# mlflow>=2.5.0
```

### 9.3 Configuration YAML Principale

```yaml
# configs/config.yaml

# =============================================================================
# CONFIGURATION G√âN√âRALE DU PROJET
# =============================================================================

project:
  name: "vehicle-damage-detection"
  version: "1.0.0"
  description: "CNN from scratch pour d√©tection de dommages v√©hicules"
  author: "Karamooo"
  
# =============================================================================
# GOOGLE DRIVE PATHS (utilis√©s dans Colab)
# =============================================================================

drive:
  root: "/content/drive/MyDrive"
  project: "/content/drive/MyDrive/ENSA_Deep_Learning"
  
# =============================================================================
# DONN√âES (sur Google Drive)
# =============================================================================

data:
  # Chemins Google Drive
  datasets_dir: "/content/drive/MyDrive/ENSA_Deep_Learning/datasets"
  raw_dir: "/content/drive/MyDrive/ENSA_Deep_Learning/datasets/raw"
  processed_dir: "/content/drive/MyDrive/ENSA_Deep_Learning/datasets/processed"
  
  # Datasets bruts
  cardd_dir: "/content/drive/MyDrive/ENSA_Deep_Learning/datasets/raw/CarDD_release/CarDD_COCO"
  stanford_dir: "/content/drive/MyDrive/ENSA_Deep_Learning/datasets/raw/stanford_cars_224/car_data"
  
  image:
    size: [224, 224]
    channels: 3
    mean: [0.485, 0.456, 0.406]
    std: [0.229, 0.224, 0.225]
  
  split:
    train: 0.70
    val: 0.15
    test: 0.15
    seed: 42
    stratified: true
  
  classes:
    - undamaged  # Label 0
    - damaged    # Label 1

# =============================================================================
# AUGMENTATION
# =============================================================================

augmentation:
  train:
    horizontal_flip:
      p: 0.5
    rotation:
      degrees: 15
    color_jitter:
      brightness: 0.2
      contrast: 0.2
      saturation: 0.1
      hue: 0.05
    random_resized_crop:
      scale: [0.8, 1.0]
  
  val: null  # Pas d'augmentation pour validation
  test: null # Pas d'augmentation pour test

# =============================================================================
# MOD√àLES
# =============================================================================

models:
  baseline:
    name: "BaselineCNN"
    num_classes: 2
    dropout_rate: 0.5
    
  deep:
    name: "DeepCNN"
    num_classes: 2
    
# =============================================================================
# ENTRA√éNEMENT
# =============================================================================

training:
  batch_size: 32
  num_epochs: 100
  num_workers: 2          # R√©duit pour Colab
  pin_memory: true
  
  optimizer:
    name: "Adam"
    lr: 0.001
    weight_decay: 0.0001
    betas: [0.9, 0.999]
  
  scheduler:
    name: "ReduceLROnPlateau"
    mode: "min"
    factor: 0.1
    patience: 5
    min_lr: 0.000001
  
  early_stopping:
    patience: 10
    min_delta: 0.0001
    monitor: "val_loss"
  
  checkpointing:
    save_best_only: true
    monitor: "val_loss"
    
# =============================================================================
# √âVALUATION
# =============================================================================

evaluation:
  metrics:
    - accuracy
    - precision
    - recall
    - f1_score
    - confusion_matrix
    
# =============================================================================
# CHEMINS OUTPUTS (sur Google Drive)
# =============================================================================

paths:
  checkpoints: "/content/drive/MyDrive/ENSA_Deep_Learning/checkpoints"
  model_a_ckpt: "/content/drive/MyDrive/ENSA_Deep_Learning/checkpoints/model_a"
  model_b_ckpt: "/content/drive/MyDrive/ENSA_Deep_Learning/checkpoints/model_b"
  outputs: "/content/drive/MyDrive/ENSA_Deep_Learning/outputs"
  logs: "/content/drive/MyDrive/ENSA_Deep_Learning/outputs/logs"
  figures: "/content/drive/MyDrive/ENSA_Deep_Learning/outputs/figures"

# =============================================================================
# REPRODUCTIBILIT√â
# =============================================================================

seed: 42
deterministic: true
```

### 9.4 Template Premi√®re Cellule Notebook (Setup Colab)

Chaque notebook doit commencer par cette cellule de setup :

```python
# ==============================================================================
# SETUP COLAB - √Ä EX√âCUTER EN PREMIER
# ==============================================================================

# 1. Monter Google Drive
from google.colab import drive
drive.mount('/content/drive')

# 2. V√©rifier le GPU
import torch
print(f"PyTorch version: {torch.__version__}")
print(f"CUDA disponible: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"GPU: {torch.cuda.get_device_name(0)}")

# 3. D√©finir les chemins (depuis paths.py ou directement)
DRIVE_ROOT = "/content/drive/MyDrive"
PROJECT_ROOT = f"{DRIVE_ROOT}/ENSA_Deep_Learning"

# Datasets
RAW_DATA_DIR = f"{PROJECT_ROOT}/datasets/raw"
PROCESSED_DATA_DIR = f"{PROJECT_ROOT}/datasets/processed"
CARDD_DIR = f"{RAW_DATA_DIR}/CarDD_release/CarDD_COCO"
STANFORD_DIR = f"{RAW_DATA_DIR}/stanford_cars_224/car_data"

# Outputs
CHECKPOINTS_DIR = f"{PROJECT_ROOT}/checkpoints"
OUTPUTS_DIR = f"{PROJECT_ROOT}/outputs"
FIGURES_DIR = f"{OUTPUTS_DIR}/figures"

# 4. Ajouter src/ au path (si le code est sur Drive ou clon√©)
import sys
# Option A: Si le code est clon√© sur Colab
# sys.path.append('/content/vehicle-damage-detection/src')

# Option B: Si le code est sur Drive
# sys.path.append(f'{DRIVE_ROOT}/vehicle-damage-detection/src')

# 5. Installer les packages manquants (si n√©cessaire)
# !pip install albumentations -q

# 6. V√©rifier que les dossiers existent
import os
print("\nüìÅ V√©rification des dossiers:")
print(f"  ‚úì Project root: {os.path.exists(PROJECT_ROOT)}")
print(f"  ‚úì Raw data: {os.path.exists(RAW_DATA_DIR)}")
print(f"  ‚úì CarDD: {os.path.exists(CARDD_DIR)}")
print(f"  ‚úì Stanford: {os.path.exists(STANFORD_DIR)}")

print("\n‚úÖ Setup Colab termin√©!")
```

---

## 10. Sp√©cifications d'Impl√©mentation

### 10.1 Conventions de Code

```python
# Style: PEP 8 avec les adaptations suivantes

# Imports
import torch                          # Standard library first
import torch.nn as nn                  # Then related packages
from torch.utils.data import DataLoader

import numpy as np                     # Third-party
import pandas as pd
from sklearn.metrics import f1_score

from src.models import BaselineCNN     # Local imports last
from src.data import VehicleDataset

# Type hints obligatoires pour les fonctions publiques
def train_epoch(
    model: nn.Module,
    dataloader: DataLoader,
    criterion: nn.Module,
    optimizer: torch.optim.Optimizer,
    device: torch.device
) -> tuple[float, float]:
    """
    Docstring Google style.
    
    Args:
        model: Le mod√®le √† entra√Æner
        dataloader: DataLoader d'entra√Ænement
        criterion: Fonction de perte
        optimizer: Optimiseur
        device: Device (CPU/GPU)
    
    Returns:
        Tuple (loss moyenne, accuracy)
    """
    pass

# Constantes en MAJUSCULES
BATCH_SIZE = 32
NUM_CLASSES = 2
```

### 10.2 Gestion des Erreurs

```python
# V√©rifications explicites
def load_image(path: str) -> torch.Tensor:
    """Charge une image avec gestion d'erreur."""
    if not os.path.exists(path):
        raise FileNotFoundError(f"Image non trouv√©e: {path}")
    
    try:
        image = Image.open(path).convert('RGB')
    except Exception as e:
        raise ValueError(f"Impossible de charger l'image {path}: {e}")
    
    return image


# Assertions pour le debug
def forward(self, x: torch.Tensor) -> torch.Tensor:
    assert x.dim() == 4, f"Expected 4D tensor, got {x.dim()}D"
    assert x.size(1) == 3, f"Expected 3 channels, got {x.size(1)}"
    # ...
```

### 10.3 Logging

```python
import logging

# Configuration du logging
def setup_logging(log_file: str = None):
    """Configure le syst√®me de logging."""
    handlers = [logging.StreamHandler()]
    
    if log_file:
        handlers.append(logging.FileHandler(log_file))
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=handlers
    )


# Utilisation
logger = logging.getLogger(__name__)

def train():
    logger.info("D√©but de l'entra√Ænement")
    logger.info(f"Batch size: {BATCH_SIZE}")
    # ...
    logger.info(f"Epoch {epoch}: loss={loss:.4f}, acc={acc:.2f}%")
```

### 10.4 Reproductibilit√©

```python
import torch
import numpy as np
import random

def set_seed(seed: int = 42):
    """
    Fixe toutes les graines al√©atoires pour reproductibilit√©.
    
    Note: Pour une reproductibilit√© totale sur GPU, ajouter:
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    """
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
```

### 10.5 Fonctionnalit√©s Secondaires

> ‚ö†Ô∏è **Note** : Ces fonctionnalit√©s sont optionnelles et ne font pas partie des crit√®res critiques d'√©valuation. Elles d√©montrent cependant une maturit√© suppl√©mentaire du projet.

#### 10.5.1 Application Flask de D√©monstration

**Objectif** : Permettre √† un utilisateur d'uploader une image de v√©hicule et recevoir une pr√©diction de dommage.

```python
# app/app.py - Structure de base
from flask import Flask, render_template, request, jsonify
from werkzeug.utils import secure_filename
import torch
from PIL import Image

app = Flask(__name__)
app.config['UPLOAD_FOLDER'] = 'uploads/'
app.config['MAX_CONTENT_LENGTH'] = 16 * 1024 * 1024  # 16MB max

# Charger le mod√®le entra√Æn√©
model = load_model('checkpoints/model_b/best_model.pth')
model.eval()

@app.route('/')
def index():
    """Page d'accueil avec formulaire d'upload."""
    return render_template('index.html')

@app.route('/predict', methods=['POST'])
def predict():
    """
    Endpoint de pr√©diction.
    
    Re√ßoit une image, applique le pr√©traitement,
    effectue la pr√©diction et retourne le r√©sultat.
    """
    if 'file' not in request.files:
        return jsonify({'error': 'Aucun fichier envoy√©'}), 400
    
    file = request.files['file']
    if file.filename == '':
        return jsonify({'error': 'Fichier vide'}), 400
    
    # Sauvegarder et traiter l'image
    filename = secure_filename(file.filename)
    filepath = os.path.join(app.config['UPLOAD_FOLDER'], filename)
    file.save(filepath)
    
    # Pr√©diction
    image = preprocess_image(filepath)
    with torch.no_grad():
        output = model(image.unsqueeze(0))
        probabilities = torch.softmax(output, dim=1)
        predicted_class = output.argmax(dim=1).item()
    
    class_names = ['undamaged', 'damaged']
    result = {
        'prediction': class_names[predicted_class],
        'confidence': probabilities[0][predicted_class].item() * 100,
        'probabilities': {
            name: prob.item() * 100 
            for name, prob in zip(class_names, probabilities[0])
        }
    }
    
    return render_template('result.html', result=result, image_path=filepath)

if __name__ == '__main__':
    app.run(debug=True, host='0.0.0.0', port=5000)
```

**Fonctionnalit√©s de l'interface** :
- Upload d'image par glisser-d√©poser ou s√©lection
- Affichage de l'image upload√©e
- Pr√©diction avec niveau de confiance
- Visualisation des probabilit√©s par classe
- Option de t√©l√©charger un rapport PDF

#### 10.5.2 G√©n√©ration Automatique de Rapports PDF

**Objectif** : G√©n√©rer un rapport de diagnostic professionnel apr√®s analyse d'une image.

```python
# app/utils/report_generator.py
from fpdf import FPDF
from datetime import datetime
import os

class DamageReportGenerator:
    """
    G√©n√®re des rapports PDF de diagnostic de dommages v√©hicules.
    
    Le rapport inclut:
    - Informations sur le v√©hicule (si fournies)
    - Image analys√©e
    - R√©sultat de la pr√©diction
    - Niveau de confiance
    - Date et heure de l'analyse
    - Recommandations
    """
    
    def __init__(self):
        self.pdf = FPDF()
        self.pdf.set_auto_page_break(auto=True, margin=15)
    
    def generate_report(
        self,
        image_path: str,
        prediction: str,
        confidence: float,
        probabilities: dict,
        vehicle_info: dict = None,
        output_path: str = None
    ) -> str:
        """
        G√©n√®re un rapport PDF complet.
        
        Args:
            image_path: Chemin vers l'image analys√©e
            prediction: Classe pr√©dite ('damaged' ou 'undamaged')
            confidence: Niveau de confiance (0-100)
            probabilities: Probabilit√©s par classe
            vehicle_info: Infos v√©hicule (optionnel)
            output_path: Chemin de sortie (auto-g√©n√©r√© si None)
        
        Returns:
            Chemin vers le fichier PDF g√©n√©r√©
        """
        self.pdf.add_page()
        
        # En-t√™te
        self._add_header()
        
        # Informations v√©hicule (si fournies)
        if vehicle_info:
            self._add_vehicle_info(vehicle_info)
        
        # Image analys√©e
        self._add_image_section(image_path)
        
        # R√©sultats de l'analyse
        self._add_results_section(prediction, confidence, probabilities)
        
        # Recommandations
        self._add_recommendations(prediction, confidence)
        
        # Pied de page
        self._add_footer()
        
        # Sauvegarde
        if output_path is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_path = f"outputs/reports/damage_report_{timestamp}.pdf"
        
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        self.pdf.output(output_path)
        
        return output_path
    
    def _add_header(self):
        """Ajoute l'en-t√™te du rapport."""
        self.pdf.set_font('Arial', 'B', 20)
        self.pdf.cell(0, 15, 'Rapport de Diagnostic V√©hicule', ln=True, align='C')
        self.pdf.set_font('Arial', '', 12)
        self.pdf.cell(0, 10, f'Date: {datetime.now().strftime("%d/%m/%Y %H:%M")}', ln=True, align='C')
        self.pdf.ln(10)
    
    def _add_results_section(self, prediction, confidence, probabilities):
        """Ajoute la section des r√©sultats."""
        self.pdf.set_font('Arial', 'B', 14)
        self.pdf.cell(0, 10, 'R√©sultats de l\'Analyse', ln=True)
        self.pdf.set_font('Arial', '', 12)
        
        # Verdict principal
        status_color = (255, 0, 0) if prediction == 'damaged' else (0, 128, 0)
        self.pdf.set_text_color(*status_color)
        self.pdf.set_font('Arial', 'B', 16)
        verdict = 'DOMMAGE D√âTECT√â' if prediction == 'damaged' else 'AUCUN DOMMAGE D√âTECT√â'
        self.pdf.cell(0, 15, verdict, ln=True, align='C')
        
        # Reset couleur
        self.pdf.set_text_color(0, 0, 0)
        self.pdf.set_font('Arial', '', 12)
        
        # Confiance
        self.pdf.cell(0, 10, f'Niveau de confiance: {confidence:.1f}%', ln=True)
        
        # Probabilit√©s d√©taill√©es
        self.pdf.ln(5)
        for class_name, prob in probabilities.items():
            self.pdf.cell(0, 8, f'  - {class_name}: {prob:.1f}%', ln=True)
    
    def _add_recommendations(self, prediction, confidence):
        """Ajoute des recommandations bas√©es sur le r√©sultat."""
        self.pdf.ln(10)
        self.pdf.set_font('Arial', 'B', 14)
        self.pdf.cell(0, 10, 'Recommandations', ln=True)
        self.pdf.set_font('Arial', '', 11)
        
        if prediction == 'damaged':
            if confidence > 90:
                rec = "Dommage clairement identifi√©. Inspection physique recommand√©e."
            elif confidence > 70:
                rec = "Dommage probable. V√©rification manuelle conseill√©e."
            else:
                rec = "R√©sultat incertain. Analyse suppl√©mentaire n√©cessaire."
        else:
            if confidence > 90:
                rec = "V√©hicule en bon √©tat apparent. Aucune action requise."
            else:
                rec = "Pas de dommage √©vident, mais v√©rification visuelle conseill√©e."
        
        self.pdf.multi_cell(0, 8, rec)
```

**Contenu du rapport** :
- En-t√™te avec logo et date
- Informations v√©hicule (plaque, mod√®le, etc.)
- Image analys√©e int√©gr√©e
- Verdict clair (DOMMAGE / PAS DE DOMMAGE)
- Niveau de confiance avec indicateur visuel
- Probabilit√©s d√©taill√©es par classe
- Recommandations automatiques
- Pied de page avec disclaimer

---

## 11. Checklist de Validation

### 11.1 Avant de Coder ‚Äî Setup Environnement

#### Google Drive
- [ ] Dossier `ENSA_Deep_Learning/` cr√©√© dans My Drive
- [ ] Sous-dossiers cr√©√©s : `datasets/raw/`, `datasets/processed/`
- [ ] Sous-dossiers cr√©√©s : `checkpoints/model_a/`, `checkpoints/model_b/`
- [ ] Sous-dossiers cr√©√©s : `outputs/figures/`, `outputs/logs/`

#### VS Code + Extension Colab
- [ ] Extension **Google Colab** install√©e dans VS Code
- [ ] Extension **Jupyter** install√©e
- [ ] Connexion Google test√©e

#### Datasets (upload sur Drive)
- [ ] **CarDD** t√©l√©charg√© (~5 GB) et upload√© dans `datasets/raw/`
- [ ] **Stanford Cars 224√ó224** t√©l√©charg√© (~500 MB) et upload√© dans `datasets/raw/`
- [ ] Structure CarDD v√©rifi√©e : `CarDD_COCO/train2017/`, `val2017/`, `test2017/`
- [ ] Dossiers inutiles ignor√©s : `annotations/`, `CarDD_SOD/`

### 11.2 Pr√©paration des Donn√©es (dans Colab)

- [ ] Google Drive mont√© dans Colab
- [ ] GPU disponible v√©rifi√© (`nvidia-smi`)
- [ ] Images Stanford collect√©es depuis tous les sous-dossiers
- [ ] √âchantillonnage de 4,000 images Stanford (seed=42)
- [ ] Dataset combin√© cr√©√© : 8,000 images (4,000 damaged + 4,000 undamaged)
- [ ] Split stratifi√© 70/15/15 appliqu√©
- [ ] Structure `processed/train/`, `val/`, `test/` cr√©√©e sur Drive
- [ ] Distribution des classes v√©rifi√©e (50/50 dans chaque split)
- [ ] Images visualis√©es (qualit√©, r√©solution)

### 11.3 Architecture Model A

- [ ] VGGBlock impl√©ment√© et test√©
- [ ] BaselineCNN impl√©ment√©
- [ ] Forward pass test√© (pas d'erreur de dimension)
- [ ] Nombre de param√®tres v√©rifi√© (~6.5M)
- [ ] Chaque choix justifi√© dans le code (commentaires)

### 11.4 Architecture Model B

- [ ] ResidualBlock impl√©ment√© avec skip connection
- [ ] Projection 1√ó1 fonctionnelle
- [ ] DeepCNN impl√©ment√©
- [ ] Forward pass test√©
- [ ] Nombre de param√®tres v√©rifi√© (~11M)
- [ ] Initialisation des poids impl√©ment√©e
- [ ] Diff√©rences avec Model A clairement document√©es

### 11.5 Pipeline d'Entra√Ænement

- [ ] Dataset PyTorch fonctionnel
- [ ] DataLoaders configur√©s (num_workers=2 pour Colab)
- [ ] Fonction de perte choisie (CrossEntropyLoss)
- [ ] Optimiseur configur√© (Adam)
- [ ] Scheduler configur√© (ReduceLROnPlateau)
- [ ] Early stopping impl√©ment√©
- [ ] Checkpointing sur Google Drive fonctionnel
- [ ] Logging des m√©triques (TensorBoard)

### 11.6 √âvaluation

- [ ] M√©triques calcul√©es correctement
- [ ] Matrice de confusion g√©n√©r√©e
- [ ] Courbes d'apprentissage trac√©es (sauv√©es sur Drive)
- [ ] Comparaison Model A vs B document√©e
- [ ] Analyse des erreurs (FP, FN) r√©alis√©e

### 11.7 Livrables Finaux

- [ ] Code propre et document√©
- [ ] Notebooks reproductibles
- [ ] README complet
- [ ] Rapport acad√©mique r√©dig√©
- [ ] Pr√©sentation PowerPoint pr√©par√©e
- [ ] Application Flask fonctionnelle (si impl√©ment√©e)
- [ ] Tous les fichiers sur GitHub

---

## 12. Glossaire Technique

| Terme | D√©finition |
|-------|------------|
| **Batch Normalization** | Normalisation des activations par mini-batch, acc√©l√®re l'entra√Ænement |
| **Champ r√©ceptif** | Zone de l'image d'entr√©e qui influence un neurone donn√© |
| **Dropout** | D√©sactivation al√©atoire de neurones pendant l'entra√Ænement (r√©gularisation) |
| **Early Stopping** | Arr√™t de l'entra√Ænement quand la validation ne s'am√©liore plus |
| **F1-Score** | Moyenne harmonique de precision et recall |
| **Feature map** | Sortie d'une couche convolutive |
| **From scratch** | Impl√©ment√© par nous, pas import√© d'une librairie |
| **Global Average Pooling** | Moyenne spatiale d'une feature map (r√©duit √† 1√ó1) |
| **Kernel/Filtre** | Matrice de poids apprise par convolution |
| **MaxPool** | Op√©ration de pooling prenant le maximum local |
| **Overfitting** | Le mod√®le m√©morise le train set au lieu de g√©n√©raliser |
| **Padding** | Ajout de z√©ros autour de l'image pour pr√©server la taille |
| **Precision** | TP / (TP + FP) ‚Äî fiabilit√© des pr√©dictions positives |
| **Recall** | TP / (TP + FN) ‚Äî capacit√© √† d√©tecter tous les positifs |
| **ReLU** | Rectified Linear Unit: max(0, x) |
| **ResNet** | Architecture avec skip connections (He et al., 2015) |
| **Skip connection** | Connexion qui "saute" des couches (x + F(x)) |
| **Stride** | Pas de d√©placement du kernel |
| **Transfer learning** | R√©utiliser un mod√®le pr√©-entra√Æn√© (INTERDIT ici) |
| **VGG** | Architecture simple avec convolutions 3√ó3 empil√©es |
| **Vanishing gradient** | Gradients qui deviennent trop petits dans les r√©seaux profonds |

---

## üìö R√©f√©rences Acad√©miques

1. **Simonyan & Zisserman (2014)** ‚Äî "Very Deep Convolutional Networks for Large-Scale Image Recognition" (VGG)
2. **He et al. (2015)** ‚Äî "Deep Residual Learning for Image Recognition" (ResNet)
3. **Ioffe & Szegedy (2015)** ‚Äî "Batch Normalization: Accelerating Deep Network Training"
4. **Srivastava et al. (2014)** ‚Äî "Dropout: A Simple Way to Prevent Neural Networks from Overfitting"

---



> **Ce PRD est la source de v√©rit√©.**  

> 1. **Respecte les contraintes acad√©miques** ‚Äî pas de mod√®les pr√©-d√©finis
> 2. **Documente chaque choix** ‚Äî le professeur veut des concepteurs
> 3. **Teste chaque composant** ‚Äî forward pass, dimensions, gradients
> 4. **Priorise la clart√©** ‚Äî code lisible > code clever
> 5. **Suis la structure** ‚Äî organisation professionnelle
>
